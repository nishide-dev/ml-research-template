"""Training script for {{ project_name }}."""

import logging
from pathlib import Path

{% if use_hydra -%}
import hydra
from hydra.utils import instantiate
from omegaconf import DictConfig, OmegaConf
{% endif -%}
{% if use_lightning -%}
import pytorch_lightning as pl
from pytorch_lightning import seed_everything
{% else -%}
import torch
{% endif -%}

logger = logging.getLogger(__name__)


{% if use_hydra -%}
@hydra.main(version_base=None, config_path="../../configs", config_name="config")
def main(cfg: DictConfig) -> None:
    """Run the main training function.

    Args:
        cfg: Hydra configuration.
    """
    logger.info("Configuration:\n%s", OmegaConf.to_yaml(cfg))

    # Set seed
    if "seed" in cfg:
        {% if use_lightning -%}
        seed_everything(cfg.seed, workers=True)
        {% else -%}
        torch.manual_seed(cfg.seed)
        {% endif -%}
        logger.info("Seed set to: %d", cfg.seed)

    {% if use_lightning -%}
    # Instantiate data module
    logger.info("Instantiating data module")
    datamodule: pl.LightningDataModule = instantiate(cfg.data)

    # Instantiate model
    logger.info("Instantiating model")
    model: pl.LightningModule = instantiate(cfg.model)

    # Instantiate logger(s)
    lightning_logger = None
    if "logger" in cfg:
        logger.info("Instantiating logger(s)")
        lightning_logger = []
        for logger_name, logger_cfg in cfg.logger.items():
            if logger_cfg is not None and "_target_" in logger_cfg:
                lightning_logger.append(instantiate(logger_cfg))
                logger.info("  - %s", logger_name)

    # Instantiate trainer
    logger.info("Instantiating trainer")
    trainer: pl.Trainer = instantiate(cfg.trainer, logger=lightning_logger)

    # Log hyperparameters
    if lightning_logger:
        for lg in lightning_logger if isinstance(lightning_logger, list) else [lightning_logger]:
            lg.log_hyperparams(OmegaConf.to_container(cfg, resolve=True))

    # Train
    logger.info("Starting training!")
    trainer.fit(model, datamodule=datamodule)

    # Test
    logger.info("Starting testing!")
    trainer.test(model, datamodule=datamodule, ckpt_path="best")

    # Print best checkpoint
    if hasattr(trainer.checkpoint_callback, "best_model_path"):
        logger.info("Best checkpoint: %s", trainer.checkpoint_callback.best_model_path)
    {% else -%}
    # TODO: Implement your custom training loop here
    logger.info("Starting training...")
    datamodule = instantiate(cfg.data)
    model = instantiate(cfg.model)
    logger.info("Training not yet implemented!")
    {% endif -%}

    logger.info("Training complete!")
{% else -%}
def main() -> None:
    """Main training function."""
    logger.info("Starting training...")
    # TODO: Implement your training logic here
    logger.info("Training complete!")
{% endif -%}


if __name__ == "__main__":
    main()
