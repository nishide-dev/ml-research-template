# Trainer configuration

{% if use_lightning and use_hydra -%}
_target_: pytorch_lightning.Trainer

# Training
max_epochs: 10
accelerator: auto
devices: auto
precision: 32
fast_dev_run: false  # Set to true for quick testing (1 batch per epoch)

# Validation
check_val_every_n_epoch: 1
val_check_interval: 1.0

# Logging
log_every_n_steps: 10
enable_progress_bar: true

# Callbacks
callbacks:
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}/checkpoints
    filename: "epoch_{epoch:03d}"
    monitor: "val/acc"
    mode: "max"
    save_top_k: 3
    save_last: true
    auto_insert_metric_name: false

  - _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/acc"
    patience: 10
    mode: "max"

  - _target_: pytorch_lightning.callbacks.RichProgressBar
    leave: true

  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "epoch"

# Reproducibility
deterministic: false
benchmark: true
{% else -%}
# Add your trainer configuration here
max_epochs: 10
{% endif -%}
